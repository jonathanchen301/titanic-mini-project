{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bfdf5c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4673c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56346",
   "metadata": {},
   "source": [
    "This notebook mainly looks at RandomForest and Gradient Boosting as a contrast between bagging and boosting methods in ensemble learning.\n",
    "\n",
    "Bagging: combine multiple models (RandomForest is a combination of multiple decision trees)\n",
    "<br>\n",
    "<br>\n",
    "Boosting: sequential ensemble method that builds models one at a time, where each new model focuses on correcting the models of previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7aad07",
   "metadata": {},
   "source": [
    "# Read Data and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38881c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "\n",
    "X, y = df.drop(columns=['Survived']), df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb31817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb90bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Pclass       891 non-null    int64  \n",
      " 2   Name         891 non-null    object \n",
      " 3   Sex          891 non-null    object \n",
      " 4   Age          714 non-null    float64\n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Ticket       891 non-null    object \n",
      " 8   Fare         891 non-null    float64\n",
      " 9   Cabin        204 non-null    object \n",
      " 10  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 76.7+ KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7114046d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a62860",
   "metadata": {},
   "source": [
    "Looks like there are some NaN ages and cabin.\n",
    "\n",
    "Simplifying the dataset a bit. The goal isn't to get the BEST predictor, but rather to play around with different preprocessing and scikit-learn's pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ccf6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aaa9f4",
   "metadata": {},
   "source": [
    "# Train / Test Split\n",
    "\n",
    "Separate 20% of the data and set aside for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a55c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703b19bb",
   "metadata": {},
   "source": [
    "# Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dab512",
   "metadata": {},
   "source": [
    "### Numeric Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61eddb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>221.7792</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.3500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass     Sex   Age      Fare Embarked\n",
       "692       3    male   NaN   56.4958        S\n",
       "481       2    male   NaN    0.0000        S\n",
       "527       1    male   NaN  221.7792        S\n",
       "855       3  female  18.0    9.3500        S\n",
       "801       2  female  31.0   26.2500        S"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0065a2",
   "metadata": {},
   "source": [
    "Age has missing values - need imputation.\n",
    "\n",
    "Scale Fare and Pclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f396e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipline\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a2e2de",
   "metadata": {},
   "source": [
    "### Categorical Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bd20197",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipeline = Pipeline([\n",
    "    ('simple_imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddff91",
   "metadata": {},
   "source": [
    "### Combine Numeric and Categorical Into One Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774297cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"Numeric\", numeric_pipeline, [\"Pclass\",\"Age\", \"Fare\"]),\n",
    "    (\"Categorical\", categorical_pipeline, [\"Sex\", \"Embarked\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d5959",
   "metadata": {},
   "source": [
    "# Full Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bf8d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", GradientBoostingClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219f178",
   "metadata": {},
   "source": [
    "# Fit Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a113d44",
   "metadata": {},
   "source": [
    "### Hyperparam Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bdf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Grid Search for Random Forest\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=100; total time=   0.2s\n",
      "[CV] END model__max_depth=None, model__min_samples_split=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=2, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=5, model__min_samples_split=5, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=2, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__max_depth=10, model__min_samples_split=5, model__n_estimators=500; total time=   0.3s\n",
      "Best Random Forest parameters: {'model__max_depth': None, 'model__min_samples_split': 5, 'model__n_estimators': 100}\n",
      "Best Random Forest CV score: 0.7569\n",
      "Performing Grid Search for Gradient Boosting\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=100; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=5, model__n_estimators=500; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=500; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=500; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=500; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=100; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=500; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=3, model__n_estimators=500; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=200; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=500; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=500; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.2, model__max_depth=5, model__n_estimators=500; total time=   0.5s\n",
      "Best Gradient Boosting parameters: {'model__learning_rate': 0.1, 'model__max_depth': 5, 'model__n_estimators': 100}\n",
      "Best Gradient Boosting CV score: 0.7460\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200, 500],\n",
    "    \"model__max_depth\": [None, 5, 10],\n",
    "    \"model__min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200, 500],\n",
    "    \"model__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"model__max_depth\": [3, 5]\n",
    "}\n",
    "\n",
    "print(\"Performing Grid Search for Random Forest\")\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Random Forest parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best Random Forest CV score: {rf_grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"Performing Grid Search for Gradient Boosting\")\n",
    "gb_grid_search = GridSearchCV(\n",
    "    gb_pipeline,\n",
    "    gb_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Gradient Boosting parameters: {gb_grid_search.best_params_}\")\n",
    "print(f\"Best Gradient Boosting CV score: {gb_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310612f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47274e",
   "metadata": {},
   "source": [
    "Best for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494f0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__max_depth': None,\n",
       " 'model__min_samples_split': 5,\n",
       " 'model__n_estimators': 100}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = rf_grid_search.best_estimator_\n",
    "gb_model = gb_grid_search.best_estimator_\n",
    "\n",
    "rf_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323416ad",
   "metadata": {},
   "source": [
    "Best for gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58a46992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__learning_rate': 0.1,\n",
       " 'model__max_depth': 5,\n",
       " 'model__n_estimators': 100}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c65429",
   "metadata": {},
   "source": [
    "Evaluation on Test Set - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dfbd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Set Results\n",
      "Random Forest F1 Score: 0.7596899224806202\n",
      "Random Forest ROC AUC: 0.8050724637681159\n"
     ]
    }
   ],
   "source": [
    "rf_test_preds = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Test Set Results\")\n",
    "print(\"Random Forest F1 Score:\", f1_score(y_test, rf_test_preds))\n",
    "print(\"Random Forest ROC AUC:\", roc_auc_score(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200dbfe",
   "metadata": {},
   "source": [
    "Evaluation on Test Set - Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a42bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Test Set Results\n",
      "Gradient Boosting F1 Score: 0.6991869918699187\n",
      "Gradient Boosting ROC AUC: 0.7615942028985508\n"
     ]
    }
   ],
   "source": [
    "gb_test_preds = gb_model.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Test Set Results\")\n",
    "print(\"Gradient Boosting F1 Score:\", f1_score(y_test, gb_test_preds))\n",
    "print(\"Gradient Boosting ROC AUC:\", roc_auc_score(y_test, gb_test_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6076e9",
   "metadata": {},
   "source": [
    "# Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "255058ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'model__max_depth': None, 'model__min_samples...</td>\n",
       "      <td>0.759690</td>\n",
       "      <td>0.805072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>{'model__learning_rate': 0.1, 'model__max_dept...</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.761594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model                                        Best Params  \\\n",
       "0      Random Forest  {'model__max_depth': None, 'model__min_samples...   \n",
       "1  Gradient Boosting  {'model__learning_rate': 0.1, 'model__max_dept...   \n",
       "\n",
       "   F1 Score   ROC AUC  \n",
       "0  0.759690  0.805072  \n",
       "1  0.699187  0.761594  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table = pd.DataFrame({\n",
    "    \"Model\": [\"Random Forest\", \"Gradient Boosting\"],\n",
    "    \"Best Params\": [rf_grid_search.best_params_, gb_grid_search.best_params_],\n",
    "    \"F1 Score\": [f1_score(y_test, rf_test_preds), f1_score(y_test, gb_test_preds)],\n",
    "    \"ROC AUC\": [roc_auc_score(y_test, rf_test_preds), roc_auc_score(y_test, gb_test_preds)]\n",
    "})\n",
    "\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa2bad",
   "metadata": {},
   "source": [
    "Random Forest gave better F1 and ROC AUC. Let's go with this model.\n",
    "\n",
    "Random Forest might have been better because this is a smaller dataset, and there is clean, interpretable features. There are also siople feature relationships and limited hyperparameteter tuning.\n",
    "\n",
    "If this had been a larger dataset with more complex and noisy features, then gradient boosting might have been a more effective method. Gradient Boosting was a bit over kill."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
